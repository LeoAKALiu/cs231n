{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture1.introduction\n",
    "> [这里是讲义](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture1.pdf)\n",
    "\n",
    "图像被称作互联网的“暗物质”（dark matter）  \n",
    "David Marr，1970s，stages of Visual Representation  \n",
    "1. input image\n",
    "2. primal sketch(edge image)\n",
    "3. 2 1/2-D sketch \n",
    "4. 3-D model  \n",
    "\n",
    "Face Detection,2001  \n",
    "Histogam of Gradients(HoG),Dalal & Triggs,2005  \n",
    "PASCAL Visual Object Challenge(20 object categories)  \n",
    "\n",
    "_现代图像识别问题是特征多，维度高，用算法经常过拟合_  \n",
    "\n",
    "*Image Net Challenge所用算法演变*  \n",
    "- Lin CVPR 2011---svm  \n",
    "- Krizhevsky NIPS 2012------cNN,Supervision[(AlexNet)](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)  \n",
    "- Szegedy arxiv 2014/Simonyan arxiv 2014-------[VGG GoogleNet](https://arxiv.org/pdf/1409.1556.pdf)  \n",
    "- Microsoft Research Asia 2015-----[152 layer Residual Networks](https://arxiv.org/pdf/1512.03385.pdf)  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture2.image classificatioon\n",
    ">[这里是讲义](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture2.pdf)\n",
    "\n",
    "## Data-Driven Approach\n",
    "1. collect a dataset of images and labels\n",
    "2. use machine learning to train a classifier\n",
    "3. evaluate the classifier on new images  \n",
    "\n",
    "## 机器学习方法用于预测分类，一般分两个函数（步骤）：\n",
    "1. 输入函数→train\n",
    "2. 输出函数→predict  \n",
    "\n",
    "## 常用分类器1：Nearest Neighbor\n",
    "1. memorize all data and labels\n",
    "2. predict the label of the most similar train image  \n",
    "\n",
    "*Distance Metric to compare images*  \n",
    "L1 distance(Manhattan distance):$$d_1(I_1,I_2)=\\sum_{P}\\left | I_1^P - I_2^P \\right |$$  \n",
    "\n",
    "训练时间复杂度O(1), 预测时间复杂度O(N)，但是我们的需求是，训练时间可以长，但是预测速度越快越好  \n",
    "_缺点_：不准确,噪声点误分类  \n",
    "_改进_：K-Nearest Neighbor，给定一个K，将最邻近的K个样本点的分类作为最终预测结果\n",
    "\n",
    "*用欧几里得距离作为Distance Meric*  \n",
    "L2(Euclidean)distance:$$d_2(I_1,I_2)= \\sqrt{\\sum_{P}(I_1^P - I_2^P)^{2}}$$  \n",
    "### L1与L2区别 \n",
    "L1依赖于所选择的坐标系，若旋转坐标系，L1距离会变化  \n",
    "L2不依赖于坐标系  \n",
    "如果输入特征有特别含义，则用L1较好，如果特征间无差别，则用L2较好 \n",
    "\n",
    "### 设置K近邻的超参数  \n",
    "1. ×选择最佳超参数K（BAD：K=1总是对训练集拟合最好）\n",
    "2. ×分为训练集和测试集（BAD：只在测试集上预测效果好，不知道未知数据预测效果如何）  \n",
    "3. √分为训练集、验证集和测试集\n",
    "4. √cross-validation（在小数据集中非常有用，但在deep learning中不常用）\n",
    "\n",
    "_K-Nearest Neighbor on images **never used**_\n",
    "- Very slow at test time\n",
    "- Distance metrics on pixels are not informative\n",
    "L2距离对样本数据变化（图像变化，如遮挡，变换）不敏感\n",
    "- curse of dimensionality\n",
    "随着维度增加，数据个数（采样点）指数级增多  \n",
    "\n",
    "### summary\n",
    "- In image classification we start with a training set of images and labels , andd must predict labels on the test set \n",
    "- The K-Neatest Neighbors classifier predicts labels based on nearest training examples\n",
    "- Distance metric and K are hyperparameters\n",
    "- Choose hyperparameters using the validation set; only run on the test set once at the very end!  \n",
    "\n",
    "## 常用分类器2：线性分类\n",
    "### Parametric Approach\n",
    "一个尺寸为32（pixels）×32（pixels）×3（RGB）的图片，转为含3072数字的Array，通过权重矩阵W，转换为10个给定分类的分值  \n",
    "$$f(x) = Wx + b$$\n",
    "\n",
    "*不需要测试集*\n",
    "\n",
    "该方法试图在高维空间用线性划分分类,但对线性不可分集无用  \n",
    "\n",
    "如何用cost function评价W的好坏，下节课讲  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture3.Loss Functions and Optimization\n",
    "\n",
    ">[这里是讲义](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf)\n",
    "\n",
    "## 评价权重矩阵W的方法\n",
    "1. 定义一个loss function量化分类的好坏\n",
    "2. 找出最小化以上函数的参数（optimization）  \n",
    "\n",
    "## Loss function\n",
    "### 一般表示：  \n",
    "假设数据集的样本表示为$\\left\\{(x_i,y_i)\\right\\}^{N}_{i=1}$  \n",
    "$x_i$为图像，$y_i$为label（int）  \n",
    "数据集的损失表示为：$$L = \\frac{1}{N} \\sum_{i} L_i(f(x_i,W),y_i)$$  \n",
    "\n",
    "### Multiclass SVM loss(Hinge Loss)：  \n",
    "$$L_i = \\sum_{j\\neq y_i}\\max\\left ( 0,s_j - s_{y_i} + 1 \\right )$$\n",
    "其中：$s=f(x_i,W)$($s_j$表示预测分类分数，$s_{y_i}$表示其他分类分数)  \n",
    "$so:$  \n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} L_i$$\n",
    "几点特性：  \n",
    "- Hinge损失不关心具体数值，只关心不同值之间的大小关系\n",
    "- Hinge损失的取值范围为$[0,-\\infty)$\n",
    "- 若初始权重矩阵得出的s≈0，去掉一个分类重新赋值计算\n",
    "- 若想降低算法对误分类的容忍度，可用平方hinge损失  \n",
    "\n",
    "#### Hinge损失函数代码\n",
    "```python\n",
    "def L_i_wectorized(x, y, W):\n",
    "    scores = W.dot(x)\n",
    "    margins = np.maximum(0, scores - score[y] + 1)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "```\n",
    "margins[y] = 0可保证迭代时跳过待计算分类（$s_j$），实现$j\\neq y_i$  \n",
    "\n",
    "### 正则项\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j\\neq y_i} \\max(0,f(x_i;W)_j - f(x_i;W)_{y_i} +1)+\\lambda R(W)$$  \n",
    "#### 常用正则项\n",
    "- L2 regularization $R(W)=\\sum_k \\sum_l W_{k,l}^2$ \n",
    "- L1 regularization $R(W)=\\sum_k \\sum_l \\left|W_{k,l}\\right|$\n",
    "- Elastic net(L1+L2) $R(W)=\\sum_k \\sum_l \\beta W_{k,l}^2 +\\left|W_{k,l}\\right|$\n",
    "- Max norm regularization\n",
    "- Dropout\n",
    "- Batch normalization\n",
    "- stochastic depth  \n",
    "\n",
    "### Softmax Classifier(Multinomial Logistic Regression)\n",
    "Softmax函数实际为normalize化的指数函数，即$softmax(x)=normalize(e^x)$  \n",
    "分类器损失函数如下：  \n",
    "$$P(Y=k|X=x_i)=\\frac{e^{s_k}}{\\sum_j e^{s_j}}\\quad\\text{其中}s=f(x_i;W)$$\n",
    "$$L_i=-logP(Y=y_i|X=x_i)$$\n",
    "$$so:L_i=-log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})$$\n",
    "\n",
    "## Optimization\n",
    "### Follow the slope\n",
    "- In 1-dimension,the slope is the derivative of a function\n",
    "- In multiple dimensions, the slope is the gradient  \n",
    "\n",
    "### Analytic Gradient\n",
    "fast,efficient,but error-pron,could be debugging through numerical gradient（which is slow, approximat, easy to write）  \n",
    "### Gradient Descent\n",
    "```python\n",
    "while True:\n",
    "    weights_grad = evaluate_gradient(loss_fun, data, weights)\n",
    "    weights += - step_size * weights_grad # perform parameter update\n",
    "```\n",
    "### Stochastic Gradient Descent(SGD)\n",
    "Full sum expensive when N is large  \n",
    "Approximate sum using a **minibatch** of examples(32/64/128 common size)   \n",
    "```python\n",
    "while True:\n",
    "    data_batch = sample_training_data(data, 256) # sample 256 examples\n",
    "    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "    weights += - step_size * weights_grad # perform parameter update  \n",
    "```\n",
    "### Linear Classification Loss Visualization\n",
    "用动画显示训练过程，可调超参数，观察收敛速度和方式\n",
    "[链接](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)  \n",
    "### 对于图片特征线性不可分\n",
    "可采用以下方法进行特征变换：\n",
    "1. Color Histogram  \n",
    "把每个色彩的pixel数累计在相应的color bar下  \n",
    "2. Histogram of Oriented Gradients(HoG)  \n",
    "3. Bag of Words  \n",
    " - Step1.Build codebook\n",
    " - Step2.Encode images  \n",
    " ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture4.Introduction to Neural Networks\n",
    ">[这里是讲义](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf)\n",
    "\n",
    "## BP算法相关\n",
    "### Chain rule\n",
    "Backpropagation中的重要方法，利用chain rule由输出节点反向计算输入节点的梯度。  \n",
    "假设三层神经网络（一输入层，两隐藏层），输入矩阵为$X = [x_1,x_2,...,x_m]$，分别经过Hidden layer1（权重矩阵$\\theta_1$，激活函数$f(z)$）,Hidden layer2（权重矩阵$\\theta_2$，激活函数$g(z)$），到达输出层（权重矩阵$\\theta_3$，激活函数$h(z)$），输出向量为$y = [y_1,y_2,...,y_n]。  \n",
    "\n",
    "即：  \n",
    "\n",
    "$$Z^{(1)} = \\theta_1\\cdot X$$  \n",
    "\n",
    "$$Z^{(2)} = \\theta_2\\cdot f(Z^{(1)})$$  \n",
    "\n",
    "$$Z^{(3)} = \\theta_3\\cdot g(Z^{(2)})$$  \n",
    "\n",
    "$$y = h(Z^{(3)})$$\n",
    "\n",
    "\n",
    "那么，在算得输出节点损失$\\delta $后，可根据chain rule反向传播到Hidden layer2的损失如下：  \n",
    "\n",
    "$$\\frac{\\partial y}{\\partial Z_2} = $$\n",
    "\n",
    "\n",
    "### forward\n",
    "由输入向前计算得到输出，保存中间变量（$Z^{(i)}$）\n",
    "### backward\n",
    "应用chain rule及中间变量，计算各步关于输入量的损失\n",
    "\n",
    "## Neural Network\n",
    "### 几种激活函数\n",
    "- Sigmoid  \n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "- tanh $$tanh(x)$$\n",
    "- ReLU (Rectified Linear Units)[Yann LeCun,2009](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf) $$\\max(0,x)$$\n",
    "- Leaky ReLU $$\\max(0.1x,x)$$\n",
    "- ELU  \n",
    "$$\\left\\{\n",
    "\\begin{aligned}\n",
    "x\\quad x\\geqslant 0 \\\\\n",
    "\\alpha (e^{x} -1)\\quad x<0\n",
    "\\end{aligned}\n",
    "\\right.$$  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture5.Convolutional Neural Networks\n",
    ">[这里是讲义](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf)  \n",
    "\n",
    "## History\n",
    "- Frank Rosenblatt, 1957, Perceptron\n",
    "- Widrow and Hoff, 1960, Adaline/Madaline\n",
    "- Rumelhar,1986, First time back-propagation became popular\n",
    "- Hinton and Salakhutdinov, 2006, [Reinvigorated research in Deep Learning](https://www.cs.toronto.edu/~hinton/science.pdf)\n",
    "- Hinton Krizhevsky and Sutskever, 2012, [First strong results:AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)  \n",
    "\n",
    "## today\n",
    "- Hinton, 2012, [Reproduced with permission](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "- Ren He and Girshick, 2015, [Faster R-CNN](https://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf)\n",
    "- Taigman, 2014, [face recognition](https://www.cs.toronto.edu/~ranzato/publications/taigman_cvpr14.pdf)  \n",
    "\n",
    "## Convolution\n",
    "卷积的定义：[百度百科](https://baike.baidu.com/item/%E5%8D%B7%E7%A7%AF/9411006?fr=aladdin&fromid=18080681&fromtitle=Convolution)\n",
    "[wikipedia](https://en.wikipedia.org/wiki/Convolution)  \n",
    "filter 和 image的元素相乘求和：  \n",
    "$$f[x,y]*g[x,y] = \\sum_{n_1=-\\infty}^{\\infty}\\sum_{n_2=-\\infty}^{\\infty} f[n_1,n_2]\\cdot g[x-n_1,y-n_2]$$  \n",
    "\n",
    "## Filter and padding\n",
    "1. 用3×3的filter卷过7×7像素的图片  \n",
    "    - stride=1 得到一个5×5的output\n",
    "    - stride=2 得到一个3×3的output\n",
    "    - stride=3 不匹配！\n",
    "    - 一般说来，卷积层尺寸为（N-F）/stride+1\n",
    "2. 用3×3边缘用0填充的filter卷过7×7像素（边缘同样用0填充）的图片\n",
    "    - stride=1 得到一个7×7的output\n",
    "    - stride=3 得到一个3×3的output\n",
    "    - 若想得到同样尺寸的卷积层：\n",
    "        - filter尺寸为3×3时，需填充（zero pad）1行像素0\n",
    "        - filter尺寸为5×5时，zero pad with 2\n",
    "        - filter尺寸为7×7时，zero pad with 3  \n",
    "        - 一般说来zero pad with（F-1）/2即可得到同样尺寸的卷积层（F为filter尺寸）  \n",
    "        \n",
    "3. 1中的padding称为‘valid padding’，2中的padding称为‘same padding’（不会丢掉edge和Corner的信息）  \n",
    "\n",
    "## Pooling layer\n",
    "- makes the representations smaller and more manageable\n",
    "- operates over each activation map independently  \n",
    "\n",
    "### max polling  \n",
    "\n",
    "### summary\n",
    "- ConvNets 是卷积层（CONV），池化层（POOL），全链接层（FC）的堆叠\n",
    "- 倾向于用更小的filter和更深的结构\n",
    "- 倾向于用更多的卷积层、更少的池化层和全链接层\n",
    "- 典型结构：[(CONV-RELU)×N-POOL]×M-(FC-RELU)×K,SOFTMAX\n",
    "    - N一般5以内，M很大，K一般2以内  \n",
    "***    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture6.Training Neural Networks1\n",
    ">[这里是讲义](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf)\n",
    "\n",
    "## Mini-batch SGD\n",
    "循环：\n",
    "- 随机取一部分（batch）样本\n",
    "- 前向传播得到损失\n",
    "- Backpropagation 计算梯度\n",
    "- 用梯度更新参数  \n",
    "\n",
    "## Activation Functions  \n",
    "\n",
    "常见激活函数：  \n",
    "\n",
    "### sigmoid\n",
    "- Squashes numbers to range[0,1]\n",
    "- 曾经最经典的激活函数，良好的解释性，有很好的性质（指数、导数），但不适用于Backpropagation  \n",
    "缺点：  \n",
    "- 使梯度‘消失’，Backpropagation中（梯度为$\\frac{\\partial{f}}{\\partial{x}}=sigmoid(1-sigmoid)$）：\n",
    "    - 当x为较小负数，梯度趋近于0\n",
    "    - 当x为0（附近）时，梯度可继续计算\n",
    "    - 当x为较大正数，梯度趋近于0  \n",
    "- Sigmoid的输出不是以0为中心\n",
    "    - 若输入为全正或全负，\n",
    "- 指数计算耗时长  \n",
    "\n",
    "### tanh(x)\n",
    "- Squashes numbers to range[-1,1]\n",
    "- 输出以0为中心（优于sigmoid）\n",
    "- 仍使梯度在较大正负值处消失  \n",
    "\n",
    "### ReLU（*最常用于cnn*）\n",
    "- 在正数区域不会‘饱和’\n",
    "- 计算快\n",
    "- 收敛速度快于$tanh(x)$和$sigmoid$ （一般快6倍）  \n",
    "缺点：\n",
    "- 输出不以0为中心\n",
    "- 在负数区依然会饱和，且梯度为0\n",
    "- 当数据位于非激活区时（负数），该部分权重也得不到更新\n",
    "    - 解决办法：初始化ReLU时，加一个小的正偏置（bias，比如0.01）  \n",
    "    \n",
    "### Leaky ReLU\n",
    "- 不会‘饱和’\n",
    "- 计算快\n",
    "- 收敛速度快于$tanh(x)$和$sigmoid$ （一般快6倍）\n",
    "- 不存在像ReLU一样的缺点  \n",
    "- Parametric Rectifier(PReLU):$f(x)=\\max{(\\alpha x,x)}$  \n",
    "\n",
    "### ELU\n",
    "- 具有ReLU的优点\n",
    "- 输出均值接近0\n",
    "- 相比Leaky ReLU，负饱和区对噪声的鲁棒性更好  \n",
    "\n",
    "### Maxout 'Neuron'[Goodfellow,2013](http://proceedings.mlr.press/v28/goodfellow13.pdf)  \n",
    "\n",
    "### 建议\n",
    "- 首选ReLU，慎重选择学习率\n",
    "- 尝试Leaky ReLU、Maxout、ELU\n",
    "- 尝试tanh（可能并不会太好）\n",
    "- **不要用sigmoid！！！！！**  \n",
    "\n",
    "## Data Preprocessing\n",
    "- 预处理之前的分类loss对权重矩阵的变化非常敏感，难于优化\n",
    "### preprocess the data\n",
    "- zero-mean\n",
    "```python\n",
    "X -= np.mean(X, axis=0)\n",
    "```\n",
    "- normalized（机器学习中常用方法，但图像处理中一般不需要）\n",
    "```python\n",
    "X /= np.std(X, axis=0)\n",
    "```\n",
    "- PCA/Whitening 与normalized一样，不常用于图像处理  \n",
    "\n",
    "### 例子\n",
    "比如CIFAR-10中的32×32×3的图像，有以下两种处理方式：\n",
    "- 减去整个图像（所有样本）的平均（AlexNet）  \n",
    "（mean image是一个32×32×3的array）  \n",
    "- 减去每个通道的平均（VGGNet）  \n",
    "\n",
    "## Weight initialization\n",
    "- small random numbers（服从高斯分布，均值为0，方差为0.01）  \n",
    "```\n",
    "W = 0.01 * np.random.randn(D,H)\n",
    "```  \n",
    "    - 小型神经网络可以，深度网络不合适（方差衰减，所有值变为0）\n",
    "    - 一个比较好的方法[‘Xavier initialization’](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.207.2059&rep=rep1&type=pdf)\n",
    "    - 另一个方法[He et al. 2015](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)\n",
    "- 初始化不合适会怎样\n",
    "    - 过大：activation 饱和（使用tanh时），梯度为0\n",
    "    - 过小：activation 趋于0，梯度为0  \n",
    "\n",
    "## Batch Normalization\n",
    "步骤：  \n",
    "1. 在各维度上独立的求出均值和方差\n",
    "2. Normalize\n",
    "3. （一般）插入全链接层和卷积层之后\n",
    "优点：  \n",
    "- improve gradient flow through the network\n",
    "- 允许更高的学习率 \n",
    "- 降低对（权重？）初始化的依赖  \n",
    "注意：\n",
    "- 在测试集上不再重新计算mean\n",
    "\n",
    "## 训练模型中出现的几点问题\n",
    "1. loss不下降或下降很慢（但准确率在上升）\n",
    "    - 学习率太低。由于softmax选择最大值的特性，可能造成准确率上升而损失不下降的现象\n",
    "2. loss返回NaN（explode）\n",
    "    - 学习率太高\n",
    "3. 学习率一般取1e-3到1e-5  \n",
    "\n",
    "## Hyperparameter Optimization\n",
    "### Cross-validation\n",
    "1. 用较小的迭代次数粗略的选出大概合适的超参数范围\n",
    "2. 增多迭代次数，继续选出精确的超参数\n",
    "    - 如果cost一直大于3倍于初始cost，说明参数错误，尽早终止  \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lecture7.Training Neural Networks2\n",
    "\n",
    ">[这里是讲义](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture7.pdf)  \n",
    "\n",
    "## 更高级的优化方法\n",
    "- SGD的问题：\n",
    "    - 如果loss在一个方向的变化慢而另一个方向上变化快，梯度下降的过程中梯度会在变化快的方向上震荡的方式下降，若是多维特征空间，这种震荡效率很低\n",
    "    - 局部最小值或‘鞍点’：在鞍点处，有些方向是下降，另外一些方向是上升的，在多维空间最为常见\n",
    "    - ‘stochastic’：效率低，噪声影响严重  \n",
    "    \n",
    "### SGD+Momentum (*SGD改进*)\n",
    "#### 传统SGD:  \n",
    "$$x_{t+1} = x_t -\\alpha\\bigtriangledown f(x_t)$$    \n",
    "\n",
    "```python\n",
    "while True:\n",
    "    dx = compute_gradient(x)\n",
    "    x += learning_rate * dx\n",
    "```\n",
    "#### SGD+Monmentum:   \n",
    "\n",
    "- 定义$v$为“速度”，记录运行过程中梯度的平均值\n",
    "- 定义$\\rho$为“摩擦力”，一般取值0.9或0.99  \n",
    "    \n",
    "$$v_{t+1}=\\rho v_t +\\bigtriangledown f(x_t)$$\n",
    "$$x_{t+1}=x_t-\\alpha v_{t+1}$$    \n",
    "\n",
    "```python\n",
    "vx = 0\n",
    "while True:\n",
    "    dx = compute_gradient(x)\n",
    "    vx = rho * vx + dx\n",
    "    x += learning_rate * vx\n",
    "```\n",
    "#### Momentum的变型：Nesterov Momentum\n",
    "- 含纠正因子，在局部最小值处，会比常规Momentum更快找到正确位置\n",
    "$$v_{t+1}=\\rho v_t-\\alpha\\bigtriangledown f(\\tilde{x_t})$$\n",
    "$$\\tilde{x_{t+1}}=\\tilde{x_t}-\\rho v_t +(1+\\rho)v_{t+1}\\\\\n",
    "=\\tilde{x_t}+v_{t+1}+\\rho(v_{t+1}-v_t)$$  \n",
    "\n",
    "```python\n",
    "dx = compute_gradient(x)\n",
    "old_v = v\n",
    "v = rho * v - learning_rate * dx\n",
    "x += -rho * old_v + (1 + rho) * v\n",
    "```  \n",
    "\n",
    "### AdaGrad(不常用）\n",
    "```python\n",
    "grad_squared = 0\n",
    "while True:\n",
    "    dx = compute_gradient(x)\n",
    "    grad_squared += dx * dx\n",
    "    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n",
    "```  \n",
    "\n",
    "### RMSProp（AdaGrad的变型）\n",
    "- 不会像Momentum产生overshoot的问题）  \n",
    "\n",
    "```python\n",
    "grad_squard = 0\n",
    "while True:\n",
    "    dx = compute_gradient(x)\n",
    "    grad_squared = decay_rate * grad_squared + (1 - decay_rate) * dx * dx\n",
    "    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n",
    "```  \n",
    "\n",
    "### Adam(AdaGrad + Momentum)\n",
    "- 融合了两个方法的优点\n",
    "- 加入了偏置项，避免first and second moment 出现除0\n",
    "- beta1 = 0.9, beta2 = 0.999, learning_rate = 1e-3或5e-4（以上参数适用于大多数模型的初始调参）  \n",
    "\n",
    "```python\n",
    "first_moment = 0\n",
    "second_moment = 0\n",
    "while True:\n",
    "    dx = compute_gradient(x)\n",
    "    # Momentum\n",
    "    first_moment = beta1 * first_moment + (1 - beta1) * dx \n",
    "    #AdaGrad/RMSProp\n",
    "    second_moment = beta2 * second_moment + (1 - beta2) * dx * dx \n",
    "    # bias crrection\n",
    "    first_unbias = first_moment / (1 - beta1 ** t)\n",
    "    second_unbias = second_moment / (1 - beta2 ** t)\n",
    "    # AdaGrad/RMSProp\n",
    "    x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7))\n",
    "```  \n",
    "### 以上方法的学习率如何设置？\n",
    "- **学习率随着迭代次数增加而衰减！**\n",
    "    - step decay：每经过一定步数，降低学习率\n",
    "    - exponential decay：$$\\alpha=\\alpha_0 e^{-kt}$$\n",
    "    - 1/t dacay:$$\\alpha=\\alpha_0/(1+kt)$$  \n",
    "- 但不要一开始就设定decay，先设置其他超参数，最后优化模型时设置学习率decay  \n",
    "- 常用于SGD+Momentum  \n",
    "\n",
    "### 一阶优化\n",
    "- 利用梯度形成线性逼近\n",
    "- 沿梯度方向近似最小化损失函数  \n",
    "\n",
    "### 二阶优化-Newton step[(牛顿下降法Wikipedia)](https://en.wikipedia.org/wiki/Newton%27s_method)\n",
    "- 利用梯度和**Hessian**形成二次逼近\n",
    "- 直接逼近**最小值**（非近似）  \n",
    "#### Newton step[(牛顿下降法Wikipedia)](https://en.wikipedia.org/wiki/Newton%27s_method)\n",
    "- 二阶泰勒展开式：  \n",
    "$$J(\\theta)\\approx J(\\theta)+(\\theta - \\theta_0)^{\\top}\\bigtriangledown_{\\theta}J(\\theta_0)+\\frac{1}{2}(\\theta-\\theta_0)^{\\top}H(\\theta-\\theta_0)$$  \n",
    "    - 解临界点更新牛顿参数\n",
    "    $$\\theta^{*}=\\theta_0 - H^{-1}\\bigtriangledown_{\\theta}J(\\theta_0)$$  \n",
    "    - 上式不含学习率，直接求解最小值。*但问题在于*，Hessian的元素数为$O(N^2)$，求逆需要$O(N^3)$时间复杂度。  \n",
    "    - 所以上述方法在深度学习中并不常用  \n",
    "    \n",
    "#### Quasi-Newton methods(改进的Newton)\n",
    "- BGFS-[Broyden–Fletcher–Goldfarb–Shanno algorithm](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "- L-BGFS-[Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS)  \n",
    "\n",
    "### 综上，Adam一般是较好的方法，适用于大部分模型；如果用全部样本进行训练，可以尝试L-BFGS（注意去除噪声）\n",
    "### Model Ensembles:Tips and Tricks\n",
    "- 训练一系列独立的模型，在测试集上取平均结果（一般会提升2%准确率）,取\n",
    "- trick:[snapshots ensembles](https://arxiv.org/pdf/1704.00109.pdf)\n",
    "- trick:[Polyak averaging](http://www.meyn.ece.ufl.edu/archive/spm_files/Courses/ECE555-2011/555media/poljud92.pdf)\n",
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    x += - learning_rate * dx\n",
    "    # use for test set\n",
    "    x_test = 0.995 * x_test + 0.005 * x\n",
    "```\n",
    "    \n",
    "## Regularization\n",
    "### Dropout\n",
    "- Dropping：在每次前向传播中，随机选择一些神经元赋值为0\n",
    "- Dropping的概率是一个超参数，一般取0.5\n",
    "- 一般发生在全链接层  \n",
    "\n",
    "```python \n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(x):\n",
    "    # x contains the data\n",
    "    # forward pass for example 3-layer neural network\n",
    "    H1 = np.maximum(0, np.dot(W1, x) + b1)\n",
    "    U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "    H1 *= U1 # drop\n",
    "    H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "    U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "    H2 *= U2\n",
    "    out = np.dot(W3, H2) + b3\n",
    "```  \n",
    "\n",
    "- 另一种解释：\n",
    "    - Dropout相当于取了样本的一部分自己用来训练，许多Dropout后的模型（在模型中做多次Dropout，相当于许多模型共享参数）最终结果用ensemble的方法得出训练结果。\n",
    "    - 每次经过binary mask的Dropout相当于一个模型\n",
    "- 在测试集里，同样乘上Dropping的概率  \n",
    "\n",
    "```python\n",
    "def predict(x):\n",
    "    # ensembled forward pass\n",
    "    H1 = np.maximum(0, np.dot(W1, x) + b1) * p # NOTE: scale the activations\n",
    "    H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations\n",
    "    out = np.dot(W3, H2) + b3\n",
    "```  \n",
    "\n",
    "### Batch Normalization\n",
    "- 一般来说只做Batch Normalization就够，如果不够，加上dropout  \n",
    "- Udacity中关于Batch Normalization的[讲解](https://github.com/leobobsix/cs231n/blob/master/Batch_Normalization.ipynb)\n",
    "\n",
    "### Data Augmentation\n",
    "- 图像翻转、随机裁切、缩放、color jitter\n",
    "-  Training ResNet:\n",
    "    - pick random L in range[256,480]\n",
    "    - resize training image, short side=L\n",
    "    - sample random 224x224 patch\n",
    "- Testing ResNet:\n",
    "    - resize image at 5 scales:{224, 256, 384, 480, 640}\n",
    "    - for each size, use 10 224x224 crops:4coners + center +flips\n",
    "- 总之一般化的操作：在训练过程中加入随机噪声，测试时去除噪声  \n",
    "\n",
    "### Fractional Max Pooling\n",
    "\n",
    "## Transfer Learning\n",
    "|            |相似的数据集    |不同的数据集    |\n",
    "|-----------------|:----------------|:----------------|\n",
    "|数据量小      |在首层用线性回归 |不好办……试试线性回归|\n",
    "|数据量巨大     |微调某些层     |微调很多层     |\n",
    "\n",
    "如果你的数据量小（小于1M）：\n",
    "- 用一个相似的大数据集，训练CNN\n",
    "- 迁移到你的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
