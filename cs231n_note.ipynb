{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture1.introduction\n",
    "图像被称作互联网的“暗物质”  \n",
    "David Marr，1970s，stages of Visual Representation  \n",
    "    1.input image\n",
    "    2.primal sketch(edge image)\n",
    "    3.2 1/2-D sketch \n",
    "    4.3-D model\n",
    "Face Detection,2001  \n",
    "Histogam of Gradients(HoG),Dalal & Triggs,2005  \n",
    "PASCAL Visual Object Challenge(20 object categories)  \n",
    "_现代图像识别问题是特征多，维度高，用算法经常过拟合_  \n",
    "\n",
    "*Image Net Challenge所用算法演变*  \n",
    "Lin CVPR 2011---svm  \n",
    "Krizhevsky NIPS 2012------cNN,Supervision(AlexNet)  \n",
    "Szegedy arxiv 2014/Simonyan arxiv 2014-------VGG GoogleNet  \n",
    "Microsoft Research Asia 2015-----152 layer Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture2.image classificatioon\n",
    "### Data-Driven Approach\n",
    "1. collect a dataset of images and labels\n",
    "2. use machine learning to train a classifier\n",
    "3. evaluate the classifier on new images  \n",
    "\n",
    "### 机器学习方法用于预测分类，，一般分两个函数（步骤）：\n",
    "1. 输入函数→train\n",
    "2. 输出函数→predict  \n",
    "\n",
    "### 常用分类器1：Nearest Neighbor\n",
    "1. memorize all data and labels\n",
    "2. predict the label of the most similar train image  \n",
    "\n",
    "*Distance Metric to compare images*  \n",
    "L1 distance(Manhattan distance):$d_1(I_1,I_2)=\\sum_{P}\\left | I_1^P - I_2^P \\right |$  \n",
    "\n",
    "训练时间复杂度O(1), 预测时间复杂度O(N)，但是我们的需求是，训练时间可以长，但是预测速度越快越好  \n",
    "_缺点_：不准确,噪声点误分类  \n",
    "_改进_：K-Nearest Neighbor，给定一个K，将最邻近的K个样本点的分类作为最终预测结果\n",
    "\n",
    "*用欧几里得距离作为Distance Meric*  \n",
    "L2(Euclidean)distance:$d_2(I_1,I_2)= \\sqrt{\\sum_{P}(I_1^P - I_2^P)^{2}}$  \n",
    "#### L1与L2区别 \n",
    "L1依赖于所选择的坐标系，若旋转坐标系，L1距离会变化  \n",
    "L2不依赖于坐标系  \n",
    "如果输入特征有特别含义，则用L1较好，如果特征间无差别，则用L2较好 \n",
    "\n",
    "#### 设置K近邻的超参数  \n",
    "1. ×选择最佳超参数K（BAD：K=1总是对训练集拟合最好）\n",
    "2. ×分为训练集和测试集（BAD：只在测试集上预测效果好，不知道未知数据预测效果如何）  \n",
    "3. √分为训练集、验证集和测试集\n",
    "4. √cross-validation（在小数据集中非常有用，但在deep learning中不常用）\n",
    "\n",
    "_K-Nearest Neighbor on images **never used**_\n",
    "- Very slow at test time\n",
    "- Distance metrics on pixels are not informative\n",
    "L2距离对样本数据变化（图像变化，如遮挡，变换）不敏感\n",
    "- curse of dimensionality\n",
    "随着维度增加，数据个数（采样点）指数级增多  \n",
    "\n",
    "#### summary\n",
    "- In image classification we start with a training set of images and labels , andd must predict labels on the test set \n",
    "- The K-Neatest Neighbors classifier predicts labels based on nearest training examples\n",
    "- Distance metric and K are hyperparameters\n",
    "- Choose hyperparameters using the validation set; only run on the test set once at the very end!  \n",
    "\n",
    "### 常用分类器2：线性分类\n",
    "#### Parametric Approach\n",
    "一个尺寸为32（pixels）×32（pixels）×3（RGB）的图片，转为含3072数字的Array，通过权重矩阵W，转换为10个给定分类的分值  \n",
    "$f(x) = Wx + b$\n",
    "\n",
    "*不需要测试集*\n",
    "\n",
    "该方法试图在高维空间用线性划分分类,但对线性不可分集无用  \n",
    "\n",
    "如何用cost function评价W的好坏，下节课讲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture3.Loss Functions and Optimization\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
