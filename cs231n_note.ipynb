{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture1.introduction\n",
    "图像被称作互联网的“暗物质”  \n",
    "David Marr，1970s，stages of Visual Representation  \n",
    "    1.input image\n",
    "    2.primal sketch(edge image)\n",
    "    3.2 1/2-D sketch \n",
    "    4.3-D model\n",
    "Face Detection,2001  \n",
    "Histogam of Gradients(HoG),Dalal & Triggs,2005  \n",
    "PASCAL Visual Object Challenge(20 object categories)  \n",
    "_现代图像识别问题是特征多，维度高，用算法经常过拟合_  \n",
    "\n",
    "*Image Net Challenge所用算法演变*  \n",
    "Lin CVPR 2011---svm  \n",
    "Krizhevsky NIPS 2012------cNN,Supervision(AlexNet)  \n",
    "Szegedy arxiv 2014/Simonyan arxiv 2014-------VGG GoogleNet  \n",
    "Microsoft Research Asia 2015-----152 layer Residual Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture2.image classificatioon\n",
    "### Data-Driven Approach\n",
    "1. collect a dataset of images and labels\n",
    "2. use machine learning to train a classifier\n",
    "3. evaluate the classifier on new images  \n",
    "\n",
    "### 机器学习方法用于预测分类，一般分两个函数（步骤）：\n",
    "1. 输入函数→train\n",
    "2. 输出函数→predict  \n",
    "\n",
    "### 常用分类器1：Nearest Neighbor\n",
    "1. memorize all data and labels\n",
    "2. predict the label of the most similar train image  \n",
    "\n",
    "*Distance Metric to compare images*  \n",
    "L1 distance(Manhattan distance):$$d_1(I_1,I_2)=\\sum_{P}\\left | I_1^P - I_2^P \\right |$$  \n",
    "\n",
    "训练时间复杂度O(1), 预测时间复杂度O(N)，但是我们的需求是，训练时间可以长，但是预测速度越快越好  \n",
    "_缺点_：不准确,噪声点误分类  \n",
    "_改进_：K-Nearest Neighbor，给定一个K，将最邻近的K个样本点的分类作为最终预测结果\n",
    "\n",
    "*用欧几里得距离作为Distance Meric*  \n",
    "L2(Euclidean)distance:$$d_2(I_1,I_2)= \\sqrt{\\sum_{P}(I_1^P - I_2^P)^{2}}$$  \n",
    "#### L1与L2区别 \n",
    "L1依赖于所选择的坐标系，若旋转坐标系，L1距离会变化  \n",
    "L2不依赖于坐标系  \n",
    "如果输入特征有特别含义，则用L1较好，如果特征间无差别，则用L2较好 \n",
    "\n",
    "#### 设置K近邻的超参数  \n",
    "1. ×选择最佳超参数K（BAD：K=1总是对训练集拟合最好）\n",
    "2. ×分为训练集和测试集（BAD：只在测试集上预测效果好，不知道未知数据预测效果如何）  \n",
    "3. √分为训练集、验证集和测试集\n",
    "4. √cross-validation（在小数据集中非常有用，但在deep learning中不常用）\n",
    "\n",
    "_K-Nearest Neighbor on images **never used**_\n",
    "- Very slow at test time\n",
    "- Distance metrics on pixels are not informative\n",
    "L2距离对样本数据变化（图像变化，如遮挡，变换）不敏感\n",
    "- curse of dimensionality\n",
    "随着维度增加，数据个数（采样点）指数级增多  \n",
    "\n",
    "#### summary\n",
    "- In image classification we start with a training set of images and labels , andd must predict labels on the test set \n",
    "- The K-Neatest Neighbors classifier predicts labels based on nearest training examples\n",
    "- Distance metric and K are hyperparameters\n",
    "- Choose hyperparameters using the validation set; only run on the test set once at the very end!  \n",
    "\n",
    "### 常用分类器2：线性分类\n",
    "#### Parametric Approach\n",
    "一个尺寸为32（pixels）×32（pixels）×3（RGB）的图片，转为含3072数字的Array，通过权重矩阵W，转换为10个给定分类的分值  \n",
    "$$f(x) = Wx + b$$\n",
    "\n",
    "*不需要测试集*\n",
    "\n",
    "该方法试图在高维空间用线性划分分类,但对线性不可分集无用  \n",
    "\n",
    "如何用cost function评价W的好坏，下节课讲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture3.Loss Functions and Optimization\n",
    "## 评价权重矩阵W的方法\n",
    "1. 定义一个loss function量化分类的好坏\n",
    "2. 找出最小化以上函数的参数（optimization）  \n",
    "\n",
    "## Loss function\n",
    "### 一般表示：  \n",
    "假设数据集的样本表示为$\\left\\{(x_i,y_i)\\right\\}^{N}_{i=1}$  \n",
    "$x_i$为图像，$y_i$为label（int）  \n",
    "数据集的损失表示为：$$L = \\frac{1}{N} \\sum_{i} L_i(f(x_i,W),y_i)$$  \n",
    "\n",
    "### Multiclass SVM loss(Hinge Loss)：  \n",
    "$$L_i = \\sum_{j\\neq y_i}\\max\\left ( 0,s_j - s_{y_i} + 1 \\right )$$\n",
    "其中：$s=f(x_i,W)$($s_j$表示预测分类分数，$s_{y_i}$表示其他分类分数)  \n",
    "$so:$  \n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} L_i$$\n",
    "几点特性：  \n",
    "- Hinge损失不关心具体数值，只关心不同值之间的大小关系\n",
    "- Hinge损失的取值范围为$[0,-\\infty)$\n",
    "- 若初始权重矩阵得出的s≈0，去掉一个分类重新赋值计算\n",
    "- 若想降低算法对误分类的容忍度，可用平方hinge损失  \n",
    "\n",
    "#### Hinge损失函数代码\n",
    "```python\n",
    "def L_i_wectorized(x, y, W):\n",
    "    scores = W.dot(x)\n",
    "    margins = np.maximum(0, scores - score[y] + 1)\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "```\n",
    "margins[y] = 0可保证迭代时跳过待计算分类（$s_j$），实现$j\\neq y_i$  \n",
    "\n",
    "### 正则项\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N}\\sum_{j\\neq y_i} \\max(0,f(x_i;W)_j - f(x_i;W)_{y_i} +1)+\\lambda R(W)$$  \n",
    "#### 常用正则项\n",
    "- L2 regularization $R(W)=\\sum_k \\sum_l W_{k,l}^2$ \n",
    "- L1 regularization $R(W)=\\sum_k \\sum_l \\left|W_{k,l}\\right|$\n",
    "- Elastic net(L1+L2) $R(W)=\\sum_k \\sum_l \\beta W_{k,l}^2 +\\left|W_{k,l}\\right|$\n",
    "- Max norm regularization\n",
    "- Dropout\n",
    "- Batch normalization\n",
    "- stochastic depth  \n",
    "\n",
    "### Softmax Classifier(Multinomial Logistic Regression)\n",
    "$$P(Y=k|X=x_i)=\\frac{e^{s_k}}{\\sum_j e^{s_j}}\\quad\\text{其中}s=f(x_i;W)$$\n",
    "$$L_i=-logP(Y=y_i|X=x_i)$$\n",
    "$$so:L_i=-log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})$$\n",
    "\n",
    "## Optimization\n",
    "### Follow the slope\n",
    "- In 1-dimension,the slope is the derivative of a function\n",
    "- In multiple dimensions, the slope is the gradient "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
